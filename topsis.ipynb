{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk rouge-score bert-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from datasets import load_dataset\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Suppress the warning\n",
    "warnings.filterwarnings(\"ignore\", message=\"Your max_length is set to *\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31a7bfcd47a8410c98acced8a9313913",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/3.36k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e778a7c2d60347c0868ddfee0314238d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/7.04k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d31acdf855e44919cb61ca16ca6afea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/2.94M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f672cd8b9ae8472da395a90bea40b59d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/14732 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f8ae769503f48878c836212339049e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/819 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d96bbab32a84248b3f932804bbc59b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/818 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"samsum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary'],\n",
       "        num_rows: 14732\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary'],\n",
       "        num_rows: 819\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary'],\n",
       "        num_rows: 818\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= dataset['test'].to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= df.sample(n= 150, replace= False).reset_index(drop= True)   \n",
    "# you may proceed with the complete test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>dialogue</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13862856</td>\n",
       "      <td>Hannah: Hey, do you have Betty's number?\\nAman...</td>\n",
       "      <td>Hannah needs Betty's number but Amanda doesn't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13729565</td>\n",
       "      <td>Eric: MACHINE!\\r\\nRob: That's so gr8!\\r\\nEric:...</td>\n",
       "      <td>Eric and Rob are going to watch a stand-up on ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13680171</td>\n",
       "      <td>Lenny: Babe, can you help me with something?\\r...</td>\n",
       "      <td>Lenny can't decide which trousers to buy. Bob ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13729438</td>\n",
       "      <td>Will: hey babe, what do you want for dinner to...</td>\n",
       "      <td>Emma will be home soon and she will let Will k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13828600</td>\n",
       "      <td>Ollie: Hi , are you in Warsaw\\r\\nJane: yes, ju...</td>\n",
       "      <td>Jane is in Warsaw. Ollie and Jane has a party....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           dialogue  \\\n",
       "0  13862856  Hannah: Hey, do you have Betty's number?\\nAman...   \n",
       "1  13729565  Eric: MACHINE!\\r\\nRob: That's so gr8!\\r\\nEric:...   \n",
       "2  13680171  Lenny: Babe, can you help me with something?\\r...   \n",
       "3  13729438  Will: hey babe, what do you want for dinner to...   \n",
       "4  13828600  Ollie: Hi , are you in Warsaw\\r\\nJane: yes, ju...   \n",
       "\n",
       "                                             summary  \n",
       "0  Hannah needs Betty's number but Amanda doesn't...  \n",
       "1  Eric and Rob are going to watch a stand-up on ...  \n",
       "2  Lenny can't decide which trousers to buy. Bob ...  \n",
       "3  Emma will be home soon and she will let Will k...  \n",
       "4  Jane is in Warsaw. Ollie and Jane has a party....  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 819 entries, 0 to 818\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        819 non-null    object\n",
      " 1   dialogue  819 non-null    object\n",
      " 2   summary   819 non-null    object\n",
      "dtypes: object(3)\n",
      "memory usage: 19.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dialogue:\n",
      " Hannah: Hey, do you have Betty's number?\n",
      "Amanda: Lemme check\n",
      "Hannah: <file_gif>\n",
      "Amanda: Sorry, can't find it.\n",
      "Amanda: Ask Larry\n",
      "Amanda: He called her last time we were at the park together\n",
      "Hannah: I don't know him well\n",
      "Hannah: <file_gif>\n",
      "Amanda: Don't be shy, he's very nice\n",
      "Hannah: If you say so..\n",
      "Hannah: I'd rather you texted him\n",
      "Amanda: Just text him ðŸ™‚\n",
      "Hannah: Urgh.. Alright\n",
      "Hannah: Bye\n",
      "Amanda: Bye bye\n",
      "\n",
      "Summary:\n",
      " Hannah needs Betty's number but Amanda doesn't have it. She needs to contact Larry.\n"
     ]
    }
   ],
   "source": [
    "print('Dialogue:\\n',df['dialogue'][0])\n",
    "print('\\nSummary:\\n',df['summary'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Metric Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_redundancy(summaries):\n",
    "    \n",
    "    total_tokens = sum(len(summary.split()) for summary in summaries)\n",
    "    unique_tokens = len(set(token for summary in summaries for token in summary.split()))\n",
    "    redundancy_score = 1 - (unique_tokens / total_tokens)\n",
    "    \n",
    "    return redundancy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(actual_summaries, pred_summaries):\n",
    "    \n",
    "    # Calculate BLEU Score\n",
    "    actual_summaries_tokenized = [[ref.split()] for ref in actual_summaries]         # tokenizing the actual summary\n",
    "    pred_summaries_tokenized = [output.split() for output in pred_summaries]         # tokenizing the predicted summary\n",
    "    bleu_score = corpus_bleu(actual_summaries_tokenized, pred_summaries_tokenized)   # comparing the tokens to calculate BLEU score\n",
    "    \n",
    "    \n",
    "    # Calculate BERT Score\n",
    "    P, R, F1 = score(actual_summaries, pred_summaries, lang='en', verbose=False)     # returns Precision, Recall and F1 score\n",
    "    bert_score = F1.mean().item()                                                    # takes the mean of F1 scores across all examples (.item() used to convert PyTorch tensor into scalar value) \n",
    "    \n",
    "    \n",
    "    # Calculate ROUGE Scores\n",
    "    rouge = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)          # creates an object 'rouge' that will be used to compute ROUGE scores with ROUGE-1, ROUGE-2, and ROUGE-L metrics, using stemming\n",
    "             \n",
    "    rouge_1_scores = []\n",
    "    rouge_2_scores = []\n",
    "    rouge_L_scores = []\n",
    "    for pred, actual in zip(pred_summaries, actual_summaries):\n",
    "        rouge_scores = rouge.score(pred, actual)                                                # returns a dictionary of mentioned ROUGE scores each of which contain precison, recall and F1 score\n",
    "        rouge_1_scores.append(rouge_scores['rouge1'].fmeasure)\n",
    "        rouge_2_scores.append(rouge_scores['rouge2'].fmeasure)\n",
    "        rouge_L_scores.append(rouge_scores['rougeL'].fmeasure)\n",
    "    \n",
    "    rouge_1_f1 = sum(rouge_1_scores) / len(rouge_1_scores)                                      # calculating the average rouge scores (considering F1 score)\n",
    "    rouge_2_f1 = sum(rouge_2_scores) / len(rouge_2_scores)\n",
    "    rouge_L_f1 = sum(rouge_L_scores) / len(rouge_L_scores)\n",
    "    \n",
    "    # Calculate Redundancy Score\n",
    "    redundancy_score = calculate_redundancy(pred_summaries)\n",
    "    \n",
    "    \n",
    "    return bleu_score, bert_score, rouge_1_f1, rouge_2_f1, rouge_L_f1, redundancy_score        # returning all the calculated metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on different models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 1. facebook/bart-large-cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "summarizer(df['dialogue'][0], max_length= 130, min_length=30, truncation= True)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions= []\n",
    "for i in range(0,len(df)):\n",
    "    pred= summarizer(df['article'][i], max_length=130, min_length=30, truncation= True)[0]['summary_text']\n",
    "    predictions.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLEU,BERT,Rouge_1,Rouge_2,Rouge_L,Redundancy = calc_metrics(df['highlights'].tolist(), predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLEU_scores= []\n",
    "BERT_scores= []\n",
    "Rouge_1_scores= []\n",
    "Rouge_2_scores= []\n",
    "Rouge_L_scores= []\n",
    "Redundancy_scores= []\n",
    "print('BLEU Score: ',BLEU)\n",
    "print('BERT Score: ',BERT)\n",
    "print('Rouge-1 Score: ',Rouge_1)\n",
    "print('Rouge-2 Score: ',Rouge_2)\n",
    "print('Rouge-L Score: ',Rouge_L)\n",
    "print('Redundancy Score: ',Redundancy)\n",
    "BLEU_scores.append(BLEU)\n",
    "BERT_scores.append(BERT)\n",
    "Rouge_1_scores.append(Rouge_1)\n",
    "Rouge_2_scores.append(Rouge_2)\n",
    "Rouge_L_scores.append(Rouge_L)\n",
    "Redundancy_scores.append(Redundancy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. sshleifer/distilbart-cnn-12-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\")\n",
    "pipe(df['article'][0], max_length= 130, min_length=30, truncation= True)[0]['summary_text']       # setting max output length to 130\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions= []\n",
    "for i in range(0,len(df)):\n",
    "    pred= pipe(df['article'][i], max_length=130, min_length=30, truncation= True)[0]['summary_text']\n",
    "    predictions.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLEU,BERT,Rouge_1,Rouge_2,Rouge_L,Redundancy = calc_metrics(df['highlights'].tolist(), predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('BLEU Score: ',BLEU)\n",
    "print('BERT Score: ',BERT)\n",
    "print('Rouge-1 Score: ',Rouge_1)\n",
    "print('Rouge-2 Score: ',Rouge_2)\n",
    "print('Rouge-L Score: ',Rouge_L)\n",
    "print('Redundancy Score: ',Redundancy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLEU_scores.append(BLEU)\n",
    "BERT_scores.append(BERT)\n",
    "Rouge_1_scores.append(Rouge_1)\n",
    "Rouge_2_scores.append(Rouge_2)\n",
    "Rouge_L_scores.append(Rouge_L)\n",
    "Redundancy_scores.append(Redundancy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. philschmid/bart-large-cnn-samsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"summarization\", model=\"philschmid/bart-large-cnn-samsum\")\n",
    "print(pipe(df['article'][0], max_length= 130, min_length=30, truncation= True)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe(df['article'][0], max_length= 130, min_length=30, truncation= True)[0]['summary_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions= []\n",
    "for i in range(0,len(df)):\n",
    "    pred= pipe(df['article'][i], max_length=130, min_length=30, truncation= True)[0]['summary_text']\n",
    "    predictions.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLEU,BERT,Rouge_1,Rouge_2,Rouge_L,Redundancy = calc_metrics(df['highlights'].tolist(), predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('BLEU Score: ',BLEU)\n",
    "print('BERT Score: ',BERT)\n",
    "print('Rouge-1 Score: ',Rouge_1)\n",
    "print('Rouge-2 Score: ',Rouge_2)\n",
    "print('Rouge-L Score: ',Rouge_L)\n",
    "print('Redundancy Score: ',Redundancy)\n",
    "BLEU_scores.append(BLEU)\n",
    "BERT_scores.append(BERT)\n",
    "Rouge_1_scores.append(Rouge_1)\n",
    "Rouge_2_scores.append(Rouge_2)\n",
    "Rouge_L_scores.append(Rouge_L)\n",
    "Redundancy_scores.append(Redundancy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. google/pegasus-cnn_dailymail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"summarization\", model=\"google/pegasus-cnn_dailymail\")\n",
    "pipe(df['article'][0], max_length= 130, min_length=30, truncation= True)[0]['summary_text'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions= []\n",
    "for i in range(0,len(df)):\n",
    "    pred= pipe(df['article'][i], max_length=130, min_length=30, truncation= True)[0]['summary_text']\n",
    "    predictions.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLEU,BERT,Rouge_1,Rouge_2,Rouge_L,Redundancy = calc_metrics(df['highlights'].tolist(), predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('BLEU Score: ',BLEU)\n",
    "print('BERT Score: ',BERT)\n",
    "print('Rouge-1 Score: ',Rouge_1)\n",
    "print('Rouge-2 Score: ',Rouge_2)\n",
    "print('Rouge-L Score: ',Rouge_L)\n",
    "print('Redundancy Score: ',Redundancy)\n",
    "BLEU_scores.append(BLEU)\n",
    "BERT_scores.append(BERT)\n",
    "Rouge_1_scores.append(Rouge_1)\n",
    "Rouge_2_scores.append(Rouge_2)\n",
    "Rouge_L_scores.append(Rouge_L)\n",
    "Redundancy_scores.append(Redundancy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. knkarthick/MEETING_SUMMARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"summarization\", model=\"knkarthick/MEETING_SUMMARY\")\n",
    "pipe(df['article'][0], max_length= 130, min_length=30, truncation= True)[0]['summary_text'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions= []\n",
    "for i in range(0,len(df)):\n",
    "    pred= pipe(df['article'][i], max_length=130, min_length=30, truncation= True)[0]['summary_text']\n",
    "    predictions.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLEU,BERT,Rouge_1,Rouge_2,Rouge_L,Redundancy = calc_metrics(df['highlights'].tolist(), predictions)\n",
    "print('BLEU Score: ',BLEU)\n",
    "print('BERT Score: ',BERT)\n",
    "print('Rouge-1 Score: ',Rouge_1)\n",
    "print('Rouge-2 Score: ',Rouge_2)\n",
    "print('Rouge-L Score: ',Rouge_L)\n",
    "print('Redundancy Score: ',Redundancy)\n",
    "BLEU_scores.append(BLEU)\n",
    "BERT_scores.append(BERT)\n",
    "Rouge_1_scores.append(Rouge_1)\n",
    "Rouge_2_scores.append(Rouge_2)\n",
    "Rouge_L_scores.append(Rouge_L)\n",
    "Redundancy_scores.append(Redundancy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topsis to find best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models= ['facebook/bart-large-cnn','sshleifer/distilbart-cnn-12-6','philschmid/bart-large-cnn-samsum','google/pegasus-cnn_dailymail','knkarthick/MEETING_SUMMARY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores= [BLEU_scores,BERT_scores,Rouge_1_scores,Rouge_2_scores,Rouge_L_scores,Redundancy_scores]\n",
    "for score in scores:\n",
    "    for i in range(len(score)):\n",
    "        score[i]= np.round(score[i],3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topsis= pd.DataFrame({\n",
    "    'Model': models,\n",
    "    'BLEU': BLEU_scores,\n",
    "    'BERT': BERT_scores,\n",
    "    'Rouge-1': Rouge_1_scores,\n",
    "    'Rouge-2': Rouge_2_scores,\n",
    "    'Rouge-L': Rouge_L_scores,\n",
    "    'Redundancy': Redundancy_scores\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights= [1,1,1,1,1,1]             # assuming equal weights (you may choose weights according to your priorities)        \n",
    "impacts= ['+','+','+','+','+','-']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(matrix):\n",
    "    norm_matrix = matrix / np.sqrt(np.sum(matrix**2, axis=0))                    # normalize the matrix\n",
    "    return norm_matrix\n",
    "\n",
    "def weighted_normalize(norm_matrix, weights):\n",
    "    weighted_norm_matrix = norm_matrix * weights                                 # calculate the weighted normalized matrix\n",
    "    return weighted_norm_matrix\n",
    "\n",
    "def ideal_best_worst(weighted_norm_matrix, impacts):\n",
    "    ideal_solution = np.max(weighted_norm_matrix, axis=0) * impacts              # calculate the ideal_best and ideal_worst solutions\n",
    "    ideal_worst_solution = np.min(weighted_norm_matrix, axis=0) * impacts\n",
    "    return ideal_solution, ideal_worst_solution\n",
    "\n",
    "def euclidean_distances(weighted_norm_matrix, ideal_solution, ideal_worst_solution):\n",
    "    dist_to_ideal = np.sqrt(np.sum((weighted_norm_matrix - ideal_solution)**2, axis=1))           # Calculate the Euclidean distances to the ideal_best and ideal_worst solutions.\n",
    "    dist_to_ideal_worst = np.sqrt(np.sum((weighted_norm_matrix - ideal_worst_solution)**2, axis=1))\n",
    "    return dist_to_ideal, dist_to_ideal_worst\n",
    "\n",
    "def performance_score(dist_to_ideal, dist_to_ideal_worst):\n",
    "    score = dist_to_ideal_worst / (dist_to_ideal + dist_to_ideal_worst)            # calculate the topsis score for each model\n",
    "    return score\n",
    "\n",
    "def topsis(matrix, weights, impacts):                                              # perform TOPSIS analysis\n",
    "    # Step 1: Normalize the decision matrix\n",
    "    norm_matrix = normalize(matrix)\n",
    "    \n",
    "    # Step 2: Calculate the weighted normalized decision matrix\n",
    "    weighted_norm_matrix = weighted_normalize(norm_matrix, weights)\n",
    "    \n",
    "    # Step 3: Determine the ideal_best and ideal_worst solutions\n",
    "    ideal_solution, ideal_worst_solution = ideal_best_worst(weighted_norm_matrix, impacts)\n",
    "    \n",
    "    # Step 4: Calculate the Euclidean distances to the ideal_best and ideal_worst solutions\n",
    "    dist_to_ideal, dist_to_ideal_worst = euclidean_distances(weighted_norm_matrix, ideal_solution, ideal_worst_solution)\n",
    "    \n",
    "    # Step 5: Calculate the performance score for each alternative/model\n",
    "    score = performance_score(dist_to_ideal, dist_to_ideal_worst)\n",
    "    \n",
    "    # Step 6: Rank the alternatives/models based on their performance scores\n",
    "    sorted_indices = np.argsort(score)[::-1]                                       # Indices of scores sorted in descending order\n",
    "    rankings = np.empty_like(sorted_indices)                                       # Create an empty array to store rankings\n",
    "    rankings[sorted_indices] = np.arange(len(score)) + 1                           # Assign ranks\n",
    "    \n",
    "    return score, rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics= df_topsis.drop('Model',axis=1)\n",
    "impacts_as_integers = [1 if impact == '+' else -1 for impact in impacts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topsis_score, rankings = topsis(df_metrics, weights, impacts_as_integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(topsis_score)):\n",
    "    topsis_score[i] = np.round(topsis_score[i], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topsis['TOPSIS Score'] = topsis_score\n",
    "df_topsis['TOPSIS Rank'] = rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topsis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
